% $ biblatex auxiliary file $
% $ biblatex version 2.3 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{cuthbertmusic21}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Cuthbert}{C.}%
     {Michael~Scott}{M.~S.}%
     {}{}%
     {}{}}%
    {{}%
     {Ariza}{A.}%
     {Christopher}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{music21}
  \strng{namehash}{CMSAC1}
  \strng{fullhash}{CMSAC1}
  \field{labelyear}{2010}
  \field{sortinit}{C}
  \field{pages}{637\bibrangedash 642}
  \field{title}{music21: A toolkit for computer-aided musicology and symbolic
  music data}
  \field{journaltitle}{International Society for Music Information Retrieval}
  \field{year}{2010}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{eck2002blues}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Eck}{E.}%
     {Douglas}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Schmidhuber}{S.}%
     {Jurgen}{J.}%
     {}{}%
     {}{}}%
  }
  \list{organization}{1}{%
    {IEEE}%
  }
  \keyw{neural networks, RNN, LSTM, jazz}
  \strng{namehash}{EDSJ1}
  \strng{fullhash}{EDSJ1}
  \field{labelyear}{2002}
  \field{sortinit}{E}
  \field{booktitle}{Neural Networks for Signal Processing, 2002. Proceedings of
  the 2002 12th IEEE Workshop on}
  \field{pages}{747\bibrangedash 756}
  \field{title}{Finding temporal structure in music: Blues improvisation with
  LSTM recurrent networks}
  \field{annotation}{%
  This article is a further development of the ideas Eck proposes in his other
  2002 paper (cite-key: eck2002structure), where he demonstrates the ability of
  LSTMs to develop an understanding of larger musical structures, such as the
  12-bar blues form. Here, Eck argues that LSTMs are also highly effective at
  learning to compose music by analyzing examples of a machine that is able to
  produce blues melodies. Eck is one of the few researchers to explore the
  realm of computational harmonic analysis in jazz, and he loves many topics
  untouched where others could explore.%
  }
  \field{year}{2002}
\endentry

\entry{franklin2006jazz}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Franklin}{F.}%
     {Judy~A}{J.~A.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {World Scientific}%
  }
  \keyw{jazz, neural networks, RNN}
  \strng{namehash}{FJA1}
  \strng{fullhash}{FJA1}
  \field{labelyear}{2006}
  \field{sortinit}{F}
  \field{number}{04}
  \field{pages}{623\bibrangedash 650}
  \field{title}{Jazz melody generation using recurrent networks and
  reinforcement learning}
  \field{volume}{15}
  \field{journaltitle}{International Journal on Artificial Intelligence Tools}
  \field{year}{2006}
\endentry

\entry{goel2014polyphonic}{incollection}{}
  \name{author}{3}{}{%
    {{}%
     {Goel}{G.}%
     {Kratarth}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Vohra}{V.}%
     {Raunaq}{R.}%
     {}{}%
     {}{}}%
    {{}%
     {Sahoo}{S.}%
     {JK}{J.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Springer}%
  }
  \keyw{machine learning, chorales, neural networks, LSTM}
  \strng{namehash}{GKVRSJ1}
  \strng{fullhash}{GKVRSJ1}
  \field{labelyear}{2014}
  \field{sortinit}{G}
  \field{booktitle}{Artificial Neural Networks and Machine Learning--ICANN
  2014}
  \field{pages}{217\bibrangedash 224}
  \field{title}{Polyphonic Music Generation by Modeling Temporal Dependencies
  Using a RNN-DBN}
  \field{year}{2014}
\endentry

\entry{goldberg2015nnlp}{report}{}
  \name{author}{1}{}{%
    {{}%
     {Goldberg}{G.}%
     {Yoav}{Y.}%
     {}{}%
     {}{}}%
  }
  \keyw{machine learning, neural networks, RNN, LSTM}
  \strng{namehash}{GY1}
  \strng{fullhash}{GY1}
  \field{labelyear}{2015}
  \field{sortinit}{G}
  \field{title}{A Primer on Neural Network Models for Natural Language
  Processing}
  \list{institution}{1}{%
    {Bar-Ilan University}%
  }
  \field{type}{techreport}
  \field{year}{2015}
\endentry

\entry{greff2015lstm}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Greff}{G.}%
     {Klaus}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Srivastava}{S.}%
     {Rupesh~Kumar}{R.~K.}%
     {}{}%
     {}{}}%
    {{}%
     {Koutnik}{K.}%
     {Jan}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Steunebrink}{S.}%
     {Bas~R.}{B.~R.}%
     {}{}%
     {}{}}%
    {{}%
     {Schmidhuber}{S.}%
     {Jurgen}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{LSTM, neural networks, chorales}
  \strng{namehash}{GK+1}
  \strng{fullhash}{GKSRKKJSBRSJ1}
  \field{labelyear}{2015}
  \field{sortinit}{G}
  \field{abstract}{%
  Several variants of the Long Short-Term Memory (LSTM) architecture for
  recurrent neural networks have been proposed since its inception in 1995. In
  recent years, these networks have become the state-of-the-art models for a
  variety of machine learning problems. This has led to a renewed interest in
  understanding the role and utility of various computational components of
  typical LSTM variants. In this paper, we present the first large-scale
  analysis of eight LSTM variants on three representative tasks: speech
  recognition, handwriting recognition, and polyphonic music modeling. The
  hyperparameters of all LSTM variants for each task were optimized separately
  using random search and their importance was assessed using the powerful
  fANOVA framework. In total, we summarize the results of 5400 experimental
  runs (about 15 years of CPU time), which makes our study the largest of its
  kind on LSTM networks. Our results show that none of the variants can improve
  upon the standard LSTM architecture significantly, and demonstrate the forget
  gate and the output activation function to be its most critical components.
  We further observe that the studied hyperparameters are virtually independent
  and derive guidelines for their efficient adjustment.%
  }
  \field{title}{LSTM: A Search Space Odyssey}
  \verb{url}
  \verb http://adsabs.harvard.edu/abs/2015arXiv150304069G
  \endverb
  \field{journaltitle}{arXiv preprint arXiv:1503.04069}
  \field{year}{2015}
\endentry

\entry{hochreiter1997long}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Hochreiter}{H.}%
     {Sepp}{S.}%
     {}{}%
     {}{}}%
    {{}%
     {Schmidhuber}{S.}%
     {J{\"u}rgen}{J.}%
     {}{}%
     {}{}}%
  }
  \name{editor}{1}{}{%
    {{}%
     {Press}{P.}%
     {MIT}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{LSTM, neural networks}
  \strng{namehash}{HSSJ1}
  \strng{fullhash}{HSSJ1}
  \field{labelyear}{1997}
  \field{sortinit}{H}
  \field{number}{8}
  \field{pages}{1735\bibrangedash 1780}
  \field{title}{Long short-term memory}
  \verb{url}
  \verb http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf
  \endverb
  \field{volume}{9}
  \field{journaltitle}{Neural Computation}
  \field{year}{1997}
\endentry

\entry{karpathy2015rnn}{misc}{}
  \name{author}{1}{}{%
    {{}%
     {Karpathy}{K.}%
     {Andrej}{A.}%
     {}{}%
     {}{}}%
  }
  \keyw{RNN, neural networks}
  \strng{namehash}{KA1}
  \strng{fullhash}{KA1}
  \field{labelyear}{2015}
  \field{sortinit}{K}
  \field{howpublished}{Blog}
  \field{title}{The Unreasonable Effectiveness of Recurrent Neural Networks}
  \verb{url}
  \verb http://karpathy.github.io/2015/05/21/rnn-effectiveness/
  \endverb
  \field{year}{2015}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{laitz2008}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Laitz}{L.}%
     {Steven~Geoffrey}{S.~G.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Oxford University Press, USA}%
  }
  \keyw{musical structure, harmonization, music theory}
  \strng{namehash}{LSG1}
  \strng{fullhash}{LSG1}
  \field{labelyear}{2012}
  \field{sortinit}{L}
  \field{edition}{3rd}
  \field{title}{The complete musician: An integrated approach to tonal theory,
  analysis, and listening}
  \field{volume}{1}
  \list{location}{1}{%
    {New York}%
  }
  \field{year}{2012}
\endentry

\entry{leaverchorale}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Leaver}{L.}%
     {Robin~A.}{R.~A.}%
     {}{}%
     {}{}}%
    {{}%
     {Marshall}{M.}%
     {Robert~L.}{R.~L.}%
     {}{}%
     {}{}}%
  }
  \keyw{musical structure, chorales}
  \strng{namehash}{LRAMRL1}
  \strng{fullhash}{LRAMRL1}
  \field{labelyear}{2015}
  \field{sortinit}{L}
  \field{title}{Chorale}
  \verb{url}
  \verb http://www.oxfordmusiconline.com.ezp-prod1.hul.harvard.edu/subscriber/a
  \verb rticle/grove/music/05652
  \endverb
  \field{journaltitle}{Grove Music Online. Oxford Music Online}
  \field{year}{2015}
\endentry

\entry{liu2014Bach}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Liu}{L.}%
     {I-Ting}{I.-T.}%
     {}{}%
     {}{}}%
    {{}%
     {Ramakrishnan}{R.}%
     {Bhiksha}{B.}%
     {}{}%
     {}{}}%
  }
  \keyw{harmonization, LSTM, neural networks}
  \strng{namehash}{LITRB1}
  \strng{fullhash}{LITRB1}
  \field{labelyear}{2014}
  \field{sortinit}{L}
  \field{abstract}{%
  We propose a framework for computer music composition that uses resilient
  propagation (RProp) and long short term memory (LSTM) recurrent neural
  network. In this paper, we show that LSTM network learns the structure and
  characteristics of music pieces properly by demonstrating its ability to
  recreate music. We also show that predicting existing music using RProp
  outperforms Back propagation through time (BPTT).%
  }
  \verb{eprint}
  \verb 1412.3191
  \endverb
  \field{title}{Bach in 2014: Music Composition with Recurrent Neural Network}
  \verb{url}
  \verb http://arxiv.org/abs/1412.3191
  \endverb
  \field{month}{12}
  \field{year}{2014}
\endentry

\entry{murphy2012machine}{book}{}
  \name{author}{1}{}{%
    {{}%
     {Murphy}{M.}%
     {Kevin~P}{K.~P.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {MIT press}%
  }
  \keyw{machine learning}
  \strng{namehash}{MKP1}
  \strng{fullhash}{MKP1}
  \field{labelyear}{2012}
  \field{sortinit}{M}
  \field{title}{Machine learning: a probabilistic perspective}
  \list{location}{1}{%
    {Cambridge, Massachusetts}%
  }
  \field{year}{2012}
\endentry

\entry{colah2015lstms}{misc}{}
  \name{author}{1}{}{%
    {{}%
     {Olah}{O.}%
     {Christopher}{C.}%
     {}{}%
     {}{}}%
  }
  \keyw{LSTM, neural networks}
  \strng{namehash}{OC1}
  \strng{fullhash}{OC1}
  \field{labelyear}{2015}
  \field{sortinit}{O}
  \field{howpublished}{Blog}
  \field{title}{Understanding LSTM Networks}
  \verb{url}
  \verb http://colah.github.io/posts/2015-08-Understanding-LSTMs/
  \endverb
  \field{annotation}{%
  In his recent blog post, Chris Olah describe a special model of recurrent
  neural networks known as LSTM Networks, or Long Short-Term Memory Networks.
  He describes the significant advances that recurrent neural networks made
  over "vanilla" (or non-recurrent) neural networks by allowing information to
  persist within the model. Theoretically, RNNs should be able to draw on
  information from previous decisions to make its current one, but in practice
  they have only proven effective when the distance between the relevant past
  information and the time frame in which it is required is small. LSTMs solved
  this problem by altering the structure of the network's hidden activation
  layers to create a pipeline of persisting information and a series of control
  gates for allowing new information to enter the pipeline. This blog post is
  aimed at machine learning researching with a general understanding of neural
  networks and are seeking a more visual and approachable introduction to
  LSTMs. While a young author, Chris is a highly accomplished researcher and
  engineer in the field, and his explanations are concise, descriptive, and
  well-researched, citing several relevant papers.%
  }
  \field{year}{2015}
  \warn{\item Invalid format of field 'date' \item Invalid format of field
  'date'}
\endentry

\lossort
\endlossort

\endinput
