% $ biblatex auxiliary file $
% $ biblatex version 2.3 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\entry{allan2005harmonising}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Allan}{A.}%
     {Moray}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Williams}{W.}%
     {Christopher~KI}{C.~K.}%
     {}{}%
     {}{}}%
  }
  \keyw{chorales, machine learning, harmonization}
  \strng{namehash}{AMWCK1}
  \strng{fullhash}{AMWCK1}
  \field{labelyear}{2005}
  \field{sortinit}{A}
  \field{pages}{25\bibrangedash 32}
  \field{title}{Harmonising chorales by probabilistic inference}
  \field{volume}{17}
  \field{journaltitle}{Advances in neural information processing systems}
  \field{year}{2005}
\endentry

\entry{breiman2001}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Breiman}{B.}%
     {Leo}{L.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Springer}%
  }
  \keyw{machine learning}
  \strng{namehash}{BL1}
  \strng{fullhash}{BL1}
  \field{labelyear}{2001}
  \field{sortinit}{B}
  \field{number}{1}
  \field{pages}{5\bibrangedash 32}
  \field{title}{Random forests}
  \field{volume}{45}
  \field{journaltitle}{Machine learning}
  \field{year}{2001}
\endentry

\entry{2014gct}{book}{}
  \name{author}{3}{}{%
    {{}%
     {Cambouropoulos}{C.}%
     {Emilios}{E.}%
     {}{}%
     {}{}}%
    {{}%
     {Kaliakatsos-Papakostas}{K.-P.}%
     {Maximos}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Tsougras}{T.}%
     {Costas}{C.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Ann Arbor, MI: Michigan Publishing, University of Michigan Library}%
  }
  \keyw{machine learning, music theory, harmonization}
  \strng{namehash}{CEKPMTC1}
  \strng{fullhash}{CEKPMTC1}
  \field{labelyear}{2014}
  \field{sortinit}{C}
  \field{title}{An idiom-independent representation of chords for computational
  music analysis and generation}
  \field{year}{2014}
\endentry

\entry{eck2002structure}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Eck}{E.}%
     {Douglas}{D.}%
     {}{}%
     {}{}}%
    {{}%
     {Schmidhuber}{S.}%
     {Juergen}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{LSTM, RNN, neural networks, jazz}
  \strng{namehash}{EDSJ1}
  \strng{fullhash}{EDSJ1}
  \field{labelyear}{2002}
  \field{sortinit}{E}
  \field{title}{A first look at music composition using lstm recurrent neural
  networks}
  \field{journaltitle}{Istituto Dalle Molle Di Studi Sull Intelligenza
  Artificiale}
  \field{annotation}{%
  Upon examining attempts at automated composition using standard recurrent
  neural networks (RNNs), the authors Eck and Schmidhuber conclude that these
  models alone fail to grasp larger musical structures, and therefore are
  unsuccessful at generating convincing musical compositions. However, a subset
  of RNNs labeled LSTMs (Long Short-Term Memory) have shown much greater
  promise due to their ability to retain information about decisions in
  previous time frames. With respect to music generation, the authors
  demonstrate its ability to successfully learn a 12-bar blues form and
  improvise melodies over those harmonies.%
  }
  \field{year}{2002}
\endentry

\entry{franklin2006jazz}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Franklin}{F.}%
     {Judy~A}{J.~A.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {World Scientific}%
  }
  \keyw{jazz, neural networks, RNN}
  \strng{namehash}{FJA1}
  \strng{fullhash}{FJA1}
  \field{labelyear}{2006}
  \field{sortinit}{F}
  \field{number}{04}
  \field{pages}{623\bibrangedash 650}
  \field{title}{Jazz melody generation using recurrent networks and
  reinforcement learning}
  \field{volume}{15}
  \field{journaltitle}{International Journal on Artificial Intelligence Tools}
  \field{year}{2006}
\endentry

\entry{greentree2005}{collection}{}
  \name{editor}{1}{}{%
    {{}%
     {Greentree}{G.}%
     {Margaret}{M.}%
     {}{}%
     {}{}}%
  }
  \keyw{chorales}
  \strng{namehash}{GM1}
  \strng{fullhash}{GM1}
  \field{labelyear}{2005}
  \field{sortinit}{G}
  \field{note}{Retrieved October 2015 from music21.}
  \field{title}{Chorales harmonized by J.S. Bach}
  \field{year}{2005}
\endentry

\entry{greff2015lstm}{article}{}
  \name{author}{5}{}{%
    {{}%
     {Greff}{G.}%
     {Klaus}{K.}%
     {}{}%
     {}{}}%
    {{}%
     {Srivastava}{S.}%
     {Rupesh~Kumar}{R.~K.}%
     {}{}%
     {}{}}%
    {{}%
     {Koutnik}{K.}%
     {Jan}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Steunebrink}{S.}%
     {Bas~R.}{B.~R.}%
     {}{}%
     {}{}}%
    {{}%
     {Schmidhuber}{S.}%
     {Jurgen}{J.}%
     {}{}%
     {}{}}%
  }
  \keyw{LSTM, neural networks, chorales}
  \strng{namehash}{GK+1}
  \strng{fullhash}{GKSRKKJSBRSJ1}
  \field{labelyear}{2015}
  \field{sortinit}{G}
  \field{abstract}{%
  Several variants of the Long Short-Term Memory (LSTM) architecture for
  recurrent neural networks have been proposed since its inception in 1995. In
  recent years, these networks have become the state-of-the-art models for a
  variety of machine learning problems. This has led to a renewed interest in
  understanding the role and utility of various computational components of
  typical LSTM variants. In this paper, we present the first large-scale
  analysis of eight LSTM variants on three representative tasks: speech
  recognition, handwriting recognition, and polyphonic music modeling. The
  hyperparameters of all LSTM variants for each task were optimized separately
  using random search and their importance was assessed using the powerful
  fANOVA framework. In total, we summarize the results of 5400 experimental
  runs (about 15 years of CPU time), which makes our study the largest of its
  kind on LSTM networks. Our results show that none of the variants can improve
  upon the standard LSTM architecture significantly, and demonstrate the forget
  gate and the output activation function to be its most critical components.
  We further observe that the studied hyperparameters are virtually independent
  and derive guidelines for their efficient adjustment.%
  }
  \field{title}{LSTM: A Search Space Odyssey}
  \verb{url}
  \verb http://adsabs.harvard.edu/abs/2015arXiv150304069G
  \endverb
  \field{journaltitle}{arXiv:1503.04069}
  \field{year}{2015}
\endentry

\entry{hild1992harmonet}{inproceedings}{}
  \name{author}{3}{}{%
    {{}%
     {Hild}{H.}%
     {Hermann}{H.}%
     {}{}%
     {}{}}%
    {{}%
     {Feulner}{F.}%
     {Johannes}{J.}%
     {}{}%
     {}{}}%
    {{}%
     {Menzel}{M.}%
     {Wolfram}{W.}%
     {}{}%
     {}{}}%
  }
  \keyw{neural networks, chorales, harmonization}
  \strng{namehash}{HHFJMW1}
  \strng{fullhash}{HHFJMW1}
  \field{labelyear}{1992}
  \field{sortinit}{H}
  \field{booktitle}{Advances in Neural Information Processing Systems}
  \field{pages}{267\bibrangedash 274}
  \field{title}{HARMONET: A neural net for harmonizing chorales in the style of
  JS Bach}
  \field{year}{1992}
\endentry

\entry{kaliakatsos2014}{inproceedings}{}
  \name{author}{2}{}{%
    {{}%
     {Kaliakatsos-Papakostas}{K.-P.}%
     {Maximos}{M.}%
     {}{}%
     {}{}}%
    {{}%
     {Cambouropoulos}{C.}%
     {Emilios}{E.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Ann Arbor, MI: Michigan Publishing, University of Michigan Library}%
  }
  \keyw{chorales, harmonization, Markov}
  \strng{namehash}{KPMCE1}
  \strng{fullhash}{KPMCE1}
  \field{labelyear}{2014}
  \field{sortinit}{K}
  \field{booktitle}{ICMC|SMC}
  \field{title}{Probabilistic harmonization with fixed intermediate chord
  constraints}
  \field{year}{2014}
\endentry

\entry{madsen2002}{article}{}
  \name{author}{2}{}{%
    {{}%
     {Madsen}{M.}%
     {Soren~Tjagvad}{S.~T.}%
     {}{}%
     {}{}}%
    {{}%
     {Jorgensen}{J.}%
     {Martin~Elmer}{M.~E.}%
     {}{}%
     {}{}}%
  }
  \keyw{chorales, neural networks}
  \strng{namehash}{MSTJME1}
  \strng{fullhash}{MSTJME1}
  \field{labelyear}{2002}
  \field{sortinit}{M}
  \field{pages}{31}
  \field{title}{Harmonisation of Bach chorales: KBS project report}
  \list{institution}{1}{%
    {University of Aarhus}%
  }
  \field{annotation}{%
  This report represents a fairly standard approach to the task of automatic
  harmonization of Bach chorales. Given the melody of a chorale, the authors
  developed a neural network to provide harmonic support for each note in the
  melody, which has been abstracted to series a quarter notes for the sake of
  evenly distributed time frames. Input features include past and previous
  elements of the melody, previously determined harmonies, and contextual
  elements of the current soprano note, such as beat strength. After
  experimenting with several different data representations for the feature
  vectors, they discovered a relatively effective solution when they
  represented the output vector as a range of MIDI notes from which the alto,
  tenor, and bass voices were selected. This paper is also somewhat dated and
  doesn't provide an adequate explanation of their neural network construction.
  However, their method is preprocessing (that is, abstracting chorales into
  quarter-note time frames) is intruiging and likely to be used in my initial
  experiments with RNNs.%
  }
  \field{year}{2002}
  \warn{\item Invalid format of field 'month'}
\endentry

\entry{mozer1994neural}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Mozer}{M.}%
     {Michael~C}{M.~C.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {Taylor \& Francis}%
  }
  \keyw{harmonization, chorales, neural networks}
  \strng{namehash}{MMC1}
  \strng{fullhash}{MMC1}
  \field{labelyear}{1994}
  \field{sortinit}{M}
  \field{number}{2-3}
  \field{pages}{247\bibrangedash 280}
  \field{title}{Neural network music composition by prediction: Exploring the
  benefits of psychoacoustic constraints and multi-scale processing}
  \field{volume}{6}
  \field{journaltitle}{Connection Science}
  \field{year}{1994}
\endentry

\entry{sun2009classification}{article}{}
  \name{author}{3}{}{%
    {{}%
     {Sun}{S.}%
     {Yanmin}{Y.}%
     {}{}%
     {}{}}%
    {{}%
     {Wong}{W.}%
     {Andrew~KC}{A.~K.}%
     {}{}%
     {}{}}%
    {{}%
     {Kamel}{K.}%
     {Mohamed~S}{M.~S.}%
     {}{}%
     {}{}}%
  }
  \list{publisher}{1}{%
    {World Scientific}%
  }
  \keyw{machine learning}
  \strng{namehash}{SYWAKKMS1}
  \strng{fullhash}{SYWAKKMS1}
  \field{labelyear}{2009}
  \field{sortinit}{S}
  \field{number}{04}
  \field{pages}{687\bibrangedash 719}
  \field{title}{Classification of imbalanced data: A review}
  \field{volume}{23}
  \field{journaltitle}{International Journal of Pattern Recognition and
  Artificial Intelligence}
  \field{year}{2009}
\endentry

\entry{petri1995bebop}{article}{}
  \name{author}{1}{}{%
    {{}%
     {Toiviainen}{T.}%
     {Petri}{P.}%
     {}{}%
     {}{}}%
  }
  \list{language}{1}{%
    {English}%
  }
  \list{publisher}{1}{%
    {University of California Press}%
  }
  \keyw{jazz, bebop, neural networks}
  \strng{namehash}{TP1}
  \strng{fullhash}{TP1}
  \field{labelyear}{1995}
  \field{sortinit}{T}
  \field{abstract}{%
  In cognitive science and research on artificial intelligence, there are two
  central paradigms: symbolic and analogical. Within the analogical paradigm,
  artificial neural networks (ANNs) have recently been successfully used to
  model and simulate cognitive phenomena. One of the most prominent features of
  ANNs is their ability to learn by example and, to a certain extent,
  generalize what they have learned. Improvisation, the art of spontaneously
  creating music while playing or singing, fundamentally has an imitative
  nature. Regardless of how much one studies and analyzes, the art of
  improvisation is learned mostly by example. Instead of memorizing explicit
  rules, the student mimics the playing of other musicians. This kind of
  learning procedure cannot be easily modeled with rule- based symbolic
  systems. ANNs, on the other hand, provide an effective means of modeling and
  simulating this kind of imitative learning. In this article, a model of jazz
  improvisation that is based on supervised learning ANNs is described. Some
  results, achieved by simulations with the model, are presented. The
  simulations show that the model is able to apply the material it has learned
  in a new context. It can even create new melodic patterns based on the
  learned patterns. This kind of adaptability is a direct consequence of the
  fact that the knowledge resides in a distributed form in the network.%
  }
  \field{issn}{07307829}
  \field{number}{4}
  \field{pages}{pp. 399\bibrangedash 413}
  \field{title}{Modeling the Target-Note Technique of Bebop-Style Jazz
  Improvisation: An Artificial Neural Network Approach}
  \verb{url}
  \verb http://www.jstor.org/stable/40285674
  \endverb
  \field{volume}{12}
  \field{journaltitle}{Music Perception: An Interdisciplinary Journal}
  \field{annotation}{%
  Petri Toivainen presents an artificial neural network (ANN) that given a
  series of chord changes is able to improvise over them in a jazz bebop style.
  The target-note technique describes provides signposts at the beginning of
  each chord change, giving the ANN targets with which to arrive at using a
  series of melodic patterns. The result is a suprisingly decent melodic
  improvisation given a small training set - consisting of Clifford Brown's
  solos on "All the Things You Are" and "Getrude's Bounce".Toivainen's paper is
  an early example of a computational approach to jazz composition.%
  }
  \field{year}{1995}
\endentry

\lossort
\endlossort

\endinput
